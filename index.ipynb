{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd5988",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "JupyterHub Stress Test - AI/ML Workload Simulation\n",
    "This notebook performs ~1-2GB of number crunching similar to AI workloads\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "NUM_ITERATIONS = 3  # Set this to run the test X number of times\n",
    "MEMORY_SCALE = 2.0  # Memory usage multiplier (1.0 = ~750MB-1GB, 2.0 = ~1.5-2GB)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"JupyterHub Stress Test - AI/ML Workload Simulation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"Memory scale: {MEMORY_SCALE}x (~{MEMORY_SCALE * 0.75:.1f}-{MEMORY_SCALE * 1.0:.1f} GB target)\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "overall_start = time.time()\n",
    "\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ITERATION {iteration + 1} of {NUM_ITERATIONS}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    iteration_start = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# Test 1: Large Matrix Operations (Neural Network Forward Pass Simulation)\n",
    "# ============================================================================\n",
    "    print(\"Test 1: Large Matrix Operations (Simulating Neural Network)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Simulate a deep neural network with large weight matrices\n",
    "    layer_sizes = [int(3000 * MEMORY_SCALE), int(2500 * MEMORY_SCALE), int(2000 * MEMORY_SCALE), int(1500 * MEMORY_SCALE), int(1000 * MEMORY_SCALE)]\n",
    "    batch_size = int(500 * MEMORY_SCALE)\n",
    "\n",
    "    print(f\"Creating neural network layers: {layer_sizes}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Initialize input data (simulating image batch)\n",
    "    X = np.random.randn(batch_size, layer_sizes[0]).astype(np.float32)\n",
    "    print(f\"Input shape: {X.shape}, Size: {X.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Forward pass through layers\n",
    "    activations = [X]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        W = np.random.randn(layer_sizes[i], layer_sizes[i+1]).astype(np.float32)\n",
    "        b = np.random.randn(layer_sizes[i+1]).astype(np.float32)\n",
    "        \n",
    "        print(f\"Layer {i+1}: W shape {W.shape}, Size: {W.nbytes / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Matrix multiplication (forward pass)\n",
    "        Z = np.dot(activations[-1], W) + b\n",
    "        \n",
    "        # ReLU activation\n",
    "        A = np.maximum(0, Z)\n",
    "        activations.append(A)\n",
    "        \n",
    "        del W, b, Z  # Clean up intermediate results\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"✓ Completed in {elapsed:.2f} seconds\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test 2: Convolutional Operations (CNN Simulation)\n",
    "# ============================================================================\n",
    "    print(\"Test 2: Convolutional Operations (Simulating CNN)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Simulate processing a batch of images\n",
    "    n_images = int(250 * MEMORY_SCALE)\n",
    "    img_height, img_width = 224, 224\n",
    "    n_channels = 3\n",
    "\n",
    "    images = np.random.randn(n_images, img_height, img_width, n_channels).astype(np.float32)\n",
    "    print(f\"Image batch shape: {images.shape}, Size: {images.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Simulate convolution with multiple filters\n",
    "    n_filters = 64\n",
    "    kernel_size = 3\n",
    "\n",
    "    for layer in range(3):\n",
    "        filters = np.random.randn(n_filters, kernel_size, kernel_size, n_channels).astype(np.float32)\n",
    "        print(f\"Conv Layer {layer+1}: {n_filters} filters, Size: {filters.nbytes / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Simulate convolution (simplified - just multiply and sum)\n",
    "        output = np.zeros((n_images, img_height-kernel_size+1, img_width-kernel_size+1, n_filters))\n",
    "        \n",
    "        for f in range(min(10, n_filters)):  # Process subset for speed\n",
    "            for i in range(0, img_height-kernel_size+1, 10):\n",
    "                for j in range(0, img_width-kernel_size+1, 10):\n",
    "                    patch = images[:, i:i+kernel_size, j:j+kernel_size, :]\n",
    "                    output[:, i, j, f] = np.sum(patch * filters[f], axis=(1,2,3))\n",
    "        \n",
    "        # Pooling operation\n",
    "        images = output[:, ::2, ::2, :]  # Max pooling simulation\n",
    "        n_channels = n_filters\n",
    "        img_height, img_width = images.shape[1], images.shape[2]\n",
    "        \n",
    "        del filters, output\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"✓ Completed in {elapsed:.2f} seconds\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test 3: Gradient Computation (Backpropagation Simulation)\n",
    "# ============================================================================\n",
    "    print(\"Test 3: Gradient Computation (Simulating Backpropagation)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Simulate computing gradients for large weight matrices\n",
    "    n_params = 10000000  # 10 million parameters\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    print(f\"Total parameters: {n_params:,}\")\n",
    "\n",
    "    # Simulate parameter tensors\n",
    "    weights = np.random.randn(n_params).astype(np.float32)\n",
    "    gradients = np.random.randn(n_params).astype(np.float32)\n",
    "\n",
    "    print(f\"Weights size: {weights.nbytes / 1024**2:.2f} MB\")\n",
    "    print(f\"Gradients size: {gradients.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Simulate gradient descent updates\n",
    "    for iter_gd in range(5):\n",
    "        # Compute gradient update\n",
    "        weights -= learning_rate * gradients\n",
    "        \n",
    "        # Add momentum (requires additional memory)\n",
    "        momentum = 0.9 * gradients + 0.1 * np.random.randn(n_params).astype(np.float32)\n",
    "        weights -= learning_rate * momentum\n",
    "        \n",
    "        if iter_gd % 1 == 0:\n",
    "            loss = np.mean(weights ** 2)\n",
    "            print(f\"GD Iteration {iter_gd+1}: Loss = {loss:.6f}\")\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"✓ Completed in {elapsed:.2f} seconds\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test 4: Embedding Operations (NLP Simulation)\n",
    "# ============================================================================\n",
    "    print(\"Test 4: Embedding Operations (Simulating NLP Model)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Simulate large vocabulary embeddings\n",
    "    vocab_size = 30000\n",
    "    embedding_dim = 512\n",
    "    sequence_length = 256\n",
    "    batch_size = 64\n",
    "\n",
    "    print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "    print(f\"Embedding dimension: {embedding_dim}\")\n",
    "    print(f\"Sequence length: {sequence_length}\")\n",
    "\n",
    "    # Create embedding matrix\n",
    "    embeddings = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "    print(f\"Embedding matrix size: {embeddings.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Simulate token sequences\n",
    "    tokens = np.random.randint(0, vocab_size, size=(batch_size, sequence_length))\n",
    "    print(f\"Token batch shape: {tokens.shape}\")\n",
    "\n",
    "    # Lookup embeddings\n",
    "    embedded = embeddings[tokens]\n",
    "    print(f\"Embedded batch size: {embedded.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Simulate attention mechanism\n",
    "    for head in range(8):\n",
    "        Q = np.random.randn(batch_size, sequence_length, embedding_dim // 8).astype(np.float32)\n",
    "        K = np.random.randn(batch_size, sequence_length, embedding_dim // 8).astype(np.float32)\n",
    "        V = np.random.randn(batch_size, sequence_length, embedding_dim // 8).astype(np.float32)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(embedding_dim // 8)\n",
    "        attention = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "        output = np.matmul(attention, V)\n",
    "        \n",
    "        del Q, K, V, scores, attention, output\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"✓ Completed in {elapsed:.2f} seconds\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test 5: Data Augmentation (Image Processing Simulation)\n",
    "# ============================================================================\n",
    "    print(\"Test 5: Data Augmentation (Image Processing)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    n_images = 500\n",
    "    img_size = 256\n",
    "    augmented_images = []\n",
    "\n",
    "    base_images = np.random.randint(0, 256, size=(n_images, img_size, img_size, 3), dtype=np.uint8)\n",
    "    print(f\"Base images size: {base_images.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Apply various augmentations\n",
    "    for i in range(5):\n",
    "        # Rotation simulation\n",
    "        rotated = np.rot90(base_images, k=i % 4, axes=(1, 2))\n",
    "        \n",
    "        # Flip simulation\n",
    "        flipped = np.flip(rotated, axis=2)\n",
    "        \n",
    "        # Brightness adjustment\n",
    "        adjusted = np.clip(flipped.astype(np.float32) * (1 + np.random.randn() * 0.2), 0, 255).astype(np.uint8)\n",
    "        \n",
    "        augmented_images.append(adjusted)\n",
    "        print(f\"Augmentation {i+1}: {adjusted.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Stack all augmented images\n",
    "    all_images = np.concatenate(augmented_images, axis=0)\n",
    "    print(f\"Total augmented dataset size: {all_images.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"✓ Completed in {elapsed:.2f} seconds\\n\")\n",
    "    \n",
    "    iteration_elapsed = time.time() - iteration_start\n",
    "    print(f\"--- Iteration {iteration + 1} total time: {iteration_elapsed:.2f} seconds ---\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "overall_elapsed = time.time() - overall_start\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STRESS TEST COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"Total runtime: {overall_elapsed:.2f} seconds ({overall_elapsed/60:.2f} minutes)\")\n",
    "print(f\"Average time per iteration: {overall_elapsed/NUM_ITERATIONS:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nApproximate peak memory usage: ~750 MB - 1 GB\")\n",
    "print(\"All tests completed successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
